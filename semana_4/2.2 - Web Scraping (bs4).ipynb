{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - Web Scraping (bs4)\n",
    "\n",
    "$$$$\n",
    "\n",
    "![scraping](images/scraping.png)\n",
    "\n",
    "$$$$\n",
    "\n",
    "Web scraping o raspado web, es una técnica utilizada mediante programas de software para extraer información de sitios web. Usualmente, estos programas simulan la navegación de un humano en la web ya sea utilizando el protocolo HTTP manualmente, o incrustando un navegador en una aplicación.\n",
    "\n",
    "El web scraping está muy relacionado con la indexación de la web, la cual indexa la información de la web utilizando un robot y es una técnica universal adoptada por la mayoría de los motores de búsqueda. Sin embargo, el web scraping se enfoca más en la transformación de datos sin estructura en la web, como el formato HTML, en datos estructurados que pueden ser almacenados y analizados en una base de datos central, en una hoja de cálculo o en alguna otra fuente de almacenamiento. Alguno de los usos del web scraping son la comparación de precios en tiendas, la monitorización de datos relacionados con el clima de cierta región, la detección de cambios en sitios webs y la integración de datos en sitios webs. \n",
    "\n",
    "En los últimos años el web scraping se ha convertido en una técnica muy utilizada dentro del sector del posicionamiento web gracias a su capacidad de generar grandes cantidades de datos para crear contenidos de calidad.\n",
    "\n",
    "Podríamos pensar que el web scraping es nuestro último recurso a falta de una API o un feed RSS. A falta de una fuente de datos, siempre podemos extraer aquello que sale por pantalla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción desde el HTML\n",
    "\n",
    "Para scrapear una página web, en primer lugar debemos conocer las estructura que tiene el HTML. Veamos la estructura básica.\n",
    "\n",
    "El HTML consiste en contenido `<etiquetado>`, es como si fueran cajas de contenido, organizado de manera jerárquica:\n",
    "\n",
    "```\n",
    "<html>\n",
    "    <head>\n",
    "        <title>Titulo de la pagina</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Cabecera</h1>\n",
    "        <p>Parrafo</p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "$$$$\n",
    "\n",
    "Las etiquetas el HTML se pueden clasificar en varios grupos, dependiendo del tipo de contenido que posea. Estos son algunos ejemplos:\n",
    "\n",
    "+ cabecera: `<h1>`, `<h2>`, `<h3>`, `<hgroup>`...\n",
    "+ texto: `<b>`, `<p>`...\n",
    "+ embebido: `<audio>`, `<img>`, `<video>`...\n",
    "+ tabular: `<table>`, `<tr>`, `<td>`, `<tbody>`...\n",
    "+ secciones: `<header>`, `<section>`, `<article>`...\n",
    "+ metadata: `<meta>`, `<title>`, `<script>`...\n",
    "\n",
    "$$$$\n",
    "\n",
    "Las etiquetas pueden tener atributos. Por ejemplo:\n",
    " \n",
    "`<div class=\"text-monospace\" id=\"name_132\", href=\"www.example.com\"> Contenido de la pagina </div>` \n",
    "\n",
    "Esta etiqueta `div` tiene los siguientes atributos:\n",
    "\n",
    "+ class: atributo con valor \"text-monospace\". La clase no es única en la página.\n",
    "+ id: atributo con valor \"name_132\". El id de una etiqueta la identifica de manera unívoca.\n",
    "+ href: atributo con valor \"www.example.com\". El href suele contener el link a otra parte de la página.\n",
    "\n",
    "Siguiendo con la analogía de las cajas, si una etiqueta de HTML es una caja, los atributos serían las pegatinas pegadas en la tapa de la caja.\n",
    "\n",
    "Conociendo cual es el contenido que queremos extraer, debemos encontrar las etiquetas dentro del HTML de la página web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la herramienta **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "from bs4 import BeautifulSoup as bs    # ambos alias son cosa mia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos Wikipedia\n",
    "\n",
    "**[Países europeos según esperanza de vida](https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos requests para extraer el HTML\n",
    "\n",
    "html=req.get(url).content   # o .text\n",
    "\n",
    "html[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parsear\n",
    "\n",
    "\n",
    "soup=bs(html, 'html.parser')\n",
    "\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(soup.find_all('table'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla=soup.find_all('table')[3]\n",
    "\n",
    "type(tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla.find_all('tr')[0].text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filas=tabla.find_all('tr')\n",
    "\n",
    "filas=[f.text.strip().split('\\n') for f in filas]\n",
    "\n",
    "filas[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "\n",
    "for f in filas:\n",
    "    \n",
    "    tmp=[]\n",
    "    \n",
    "    for palabra in f:\n",
    "        \n",
    "        if palabra!='':\n",
    "            tmp.append(palabra)\n",
    "            \n",
    "    final.append(tmp)\n",
    "    \n",
    "final[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [[palabra for palabra in f if palabra!=''] for f in filas]\n",
    "\n",
    "final[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "col_names = filas[0]\n",
    "\n",
    "data = filas[1:]\n",
    "\n",
    "\n",
    "df=pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto es solo para pandas\n",
    "\n",
    "df_c = pd.read_clipboard()    # ctrl+V\n",
    "\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo geolocalización por IP\n",
    "\n",
    "https://tools.keycdn.com/geo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Dónde estoy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://tools.keycdn.com/geo?host=2.136.116.82'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "sopa = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sopa.find('div', id='geoResult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sopa.find('div', class_='mt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sopa.find('div', {'id': 'geoResult', 'class': 'mt-4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla=sopa.find('div', {'id': 'geoResult', 'class': 'mt-4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla.find_all('dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla.find_all('dd')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[e.text for e in tabla.find_all('dd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion=[e.text for e in tabla.find_all('dd')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo una funcion para todo el proceso, segun ip\n",
    "\n",
    "\n",
    "def encontrar(ip):\n",
    "    \n",
    "    url = f'https://tools.keycdn.com/geo?host={ip}'\n",
    "    \n",
    "    html = req.get(url).text\n",
    "\n",
    "    sopa = bs(html, 'html.parser')\n",
    "    \n",
    "    tabla=sopa.find('div', {'id': 'geoResult', 'class': 'mt-4'})\n",
    "    \n",
    "    conexion=[e.text for e in tabla.find_all('dd')]\n",
    "    \n",
    "    return conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encontrar('168.123.4.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encontrar('46.222.34.200')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo LinkedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.linkedin.com/jobs/search/?currentJobId=3443970867&keywords=analista%20de%20datos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "sopa = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tarjetas = sopa.find('ul', class_='jobs-search__results-list')\n",
    "\n",
    "type(tarjetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(tarjetas.find_all('li'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curro=tarjetas.find_all('li')[0]\n",
    "\n",
    "titulo=curro.find('h3').text.strip()\n",
    "\n",
    "empresa = curro.find('h4').text.strip()\n",
    "\n",
    "link_curro = curro.find('a').attrs['href']\n",
    "\n",
    "link_comp = curro.find('h4').find('a').attrs['href']\n",
    "\n",
    "pais = curro.find('span', class_=\"job-search-card__location\").text.strip()\n",
    "\n",
    "fecha = curro.find('time').attrs['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'titulo': titulo,\n",
    "\n",
    "'empresa': empresa,\n",
    "\n",
    "'link_curro': link_curro,\n",
    "\n",
    "'link_comp': link_comp, \n",
    "\n",
    "'pais': pais, \n",
    "\n",
    "'fecha': fecha}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def linkedin(num_pages, keywords, loc, n_secs):\n",
    "\n",
    "    URL = 'https://www.linkedin.com/jobs/search/'\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(num_pages):\n",
    "\n",
    "        scrape_url = ''.join([URL,  # url principal\n",
    "                              f'?keywords={keywords}',\n",
    "                              '&geoId=105646813'\n",
    "                              f'&location={loc}',\n",
    "                              f'&f_TPR={n_secs}',\n",
    "                              f'&start={i*25}'\n",
    "                             ])\n",
    "\n",
    "        print(scrape_url)\n",
    "\n",
    "        html = req.get(url).text\n",
    "\n",
    "        sopa = bs(html, 'html.parser')\n",
    "\n",
    "        tarjetas = sopa.find('ul', class_='jobs-search__results-list')\n",
    "\n",
    "\n",
    "        for curro in tarjetas:  # bucle sobre los li, las tarjetas de cada curro\n",
    "            \n",
    "            try:\n",
    "                titulo=curro.find('h3').text.strip()\n",
    "\n",
    "                empresa = curro.find('h4').text.strip()\n",
    "\n",
    "                link_curro = curro.find('a').attrs['href']\n",
    "\n",
    "                link_comp = curro.find('h4').find('a').attrs['href']\n",
    "\n",
    "                pais = curro.find('span', class_=\"job-search-card__location\").text.strip()\n",
    "\n",
    "                fecha = curro.find('time').attrs['datetime']\n",
    "\n",
    "\n",
    "                data.append({'titulo': titulo,\n",
    "                             'empresa': empresa,\n",
    "                             'link_curro': link_curro,\n",
    "                             'link_comp': link_comp, \n",
    "                             'pais': pais, \n",
    "                             'fecha': fecha})\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=linkedin(5, 'analista%20de%20datos', 'spain', 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(df.to_html(render_links=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
